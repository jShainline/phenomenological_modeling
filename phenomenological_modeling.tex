\documentclass[twocolumn]{article}

\usepackage{geometry}
\geometry{textwidth = 18cm,textheight = 24cm}

\usepackage{cite}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{braket}
\usepackage{textcomp}
%\usepackage{lmodern}
\usepackage{authblk}
\usepackage{datetime}
\usepackage{gensymb}
\usepackage{wrapfig}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{booktabs}
%\usepackage{appendix}

%\usepackage[switch,columnwise]{lineno}
%\linenumbers

\newcommand{\onlinecite}[1]{\hspace{-1 ex} \nocite{#1}\citenum{#1}} 

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}
  
\title{Modeling Loop Neurons as Systems of Leaky Integrators}
\author[1]{\Large{Saeed Khan, Alexander N. Tait, and Jeffrey M. Shainline}
\\
\textit{\large{National Institute of Standards and Technology}}
\\
\vspace{-0.2em}
\textit{\large{325 Broadway, Boulder, CO, USA, 80305}}
\\
\vspace{-0.2em}
\textit{\large{jeffrey.shainline@nist.gov}}
}
\date{\today}%\today

\begin{document}

\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}

Superconducting optoelectronic loop neurons are a class of circuits potentially conducive to realizing networks for large-scale artificial cognition. These circuits employ superconducting components including single-photon detectors, Josephson junctions, and transformers. To date, all simulations of loop neurons have used first-principles circuit analysis to model the behavior of synapses, dendrites, and neurons. These circuit models are computationally inefficient and leave opaque the relationship between loop neurons and other neuron models employed in computational neuroscience and cognitive computing. Here we introduce a modeling framework that captures the behavior of the relevant synaptic, dendritic, and neuronal circuits at a phenomenological level without resorting to full solutions to the circuit equations. In this framework, each synapse and dendrite is discovered to obey a single leaky-integrator ordinary differential equation, while a neuron with adaptive refraction is modeled as two dendrites in a feedback configuration. We quantify the accuracy of the phenomenological model relative to circuit simulations, and find that the approach not only reduces the total number of differential equations in the coupled system, but also allows an increase in the numerical time step by a factor of 100. We demonstrate the use of the model with several basic examples. The net increase in computational efficiency enables future simulation of large, interconnected networks, while the formulation in terms of well-known leaky integrators provides a connection to a large body on work in applied mathematics and computational neuroscience, which aids in development of intuition for the operation of future superconducting optoelectronic networks.

    \vspace{3em}
    \end{abstract}
  \end{@twocolumnfalse}
]

\setcounter{tocdepth}{1}
\setcounter{secnumdepth}{4}
%\tableofcontents

\section{\label{sec:introduction}Introduction}

% this introduction is too long-winded
Comprehensive knowledge of large-scale neural systems is a grand scientific challenge with relevance to neuroscience, artificial intelligence, and the physics of critical systems \cite{elst2012,ized2008,br2017,fama2019,eide2019}. The lack of an efficient, artificial, brain-scale experimental test bed is a barrier to progress. Experiments with living systems bring challenges, including the difficulty of obtaining data from large numbers of individual neurons, let alone synapses. Data acquired at the scale of the entire network has low resolution, while it is difficult to obtain data with high spatial and temporal resolution at the device and network levels simultaneously. The invasive nature of experiments on functioning systems is also problematic. The ability to adjust device, circuit, and network parameters of biological neural systems is limited, limiting the potential of controlled experiments. An artificial platform capable of realizing networks at the scale and complexity of the brains of intelligent organisms would be a tool of supreme scientific utility.

Neuromorphic hardware based on silicon microelectronics has a great deal to offer in this regard \cite{me1989,lide2015,chba2003,inch2006,voma2007,inli2011,cryu2012,cage2013,pfgr2013,bega2014,fu2016,moqi2018}. Yet challenges remain. Silicon neural circuits cannot directly communicate via axons in the manner of biological neurons, so communication is carried out over shared circuits comprising an interconnection network. Bottlenecks result that make information integration across space and time less efficient than biological neural systems. Additionally, adaptation and learning are hindered by difficulties with mechanisms for storing and writing a memory state in devices monolithically integrated with the MOSFETs performing neural computations. While the decades of progress developing silicon neural circuits have achieved impressive gains, as a test bed for cognition, alternative hardware may bring new benefits.

Many proposals have arisen to explore alternative hardware for cognition, including using optical \cite{psfa1985,faps1985,abps1987,waps1993,safi1995,mosc1997,hoiz2000,moho2000,rokr2009,krfo2011,dusc2012,nash2013,prsh2017,shha2017,pena2018,tafe2019} and magnetic \cite{} phenomena as well as phase-change materials \cite{chsa2019,feyo2019}. It has been argued that using optical pulses to represent action potentials may alleviate the need for a shared communication infrastructure and the associated demands on address storage and memory access \cite{shbu2017,sh2019}. The use of superconducting circuits bring the thresholding nonlinearity of Josephson junctions \cite{vatu1998,ka1999} as well as memory retention and plasticity through the storage of flux in superconducting loops \cite{sh2019}. By using superconducting detectors \cite{nata2012} in conjunction with optical pulses, neurons can signal to synapses at the single-photon level. In conjunction with minimal static power dissipation, the energy efficiency of such systems may be conducive to very large artificial cognitive systems. Although the constituent devices dwarf their biological counterparts, communication at the speed of light enables a very large neuronal pool \cite{sh2019,sh2018_ICRC}. The apparent circuit complexity and adaptability are intriguing for scientific study.

% consider starting close to here
While the components of these superconducting optoelectronic networks (SOENs) have been demonstrated, full circuits have not. Prior to undertaking the full effort and expense of realizing these complex systems, it is prudent to gain confidence they are indeed ripe for further investigation. This confidence can be gained through simulations of device, circuit, and system behavior using conventional numerical methods. The computations performed by superconducting optoelectronic loop neurons are accomplished by circuits comprising superconducting single-photon detectors, Josephson junctions, and mutual inductors. These devices are most commonly modeled with SPICE circuit simulations carried out on a picosecond time scale to accurately capture the dynamics of the Josephson junctions. Simulation of networks of large numbers of these neurons become computationally intensive. From the perspective of the neural system, the picosecond dynamics of the JJs are not the primary interest, and one would prefer to treat each synapse, dendrite, and neuron as an input-output device with a model that accurately captures the circuit dynamics on the nanosecond to microsecond time scales while not explicitly treating the picosecond behavior of the underlying circuit elements.

Here we introduce a phenomenological model of loop neurons and their constitutive elements that accurately captures the transfer characteristics of the circuits without solving the underlying circuit equations. Each synapse and dendrite is treated with a single, first-order ordinary differential equation that describes the output of the element as a function of its inputs and instantaneous internal state. These equations take the standard form of a leaky integrator with a nonlinear driving term. Each neuron performs passive summation of its synaptic and dendritic inputs. When the sum of the inputs reaches threshold, the neuron produces a spike. Refraction is treated with the same formalism as a strong inhibitory synapse receiving the neuron's spikes and directly contacting the neuron's cell body. When full circuit simulations are conducted, each synapse or dendrite require solving a system of seven equations at each time step, and time steps of 1\,ps are typical. The model presented here reduces the number of equations for each synapse or dendrite to one and allows simulation with time steps of 1\,ns. 

We begin by motivating the form of the model based on circuit considerations. We then describe the means by which the form of the driving term in the leaky integrator is obtained. Error is quantified by comparison with full circuit simulations, and convergence is investigated as a function of time step size. Point neurons are considered to test the synaptic equations. Dendrites are treated next, and the similarity between synaptic and dendritic circuits permits a treatment with a similar form of leaky integrator equation. Dendritic accuracy is quantified, and an example of a neuron with a dendritic arbor is presented to illustrate the utility of the model. Throughout the work we compare and contrast loop neurons with their biological counterparts. 

\section{\label{sec:loop_neurons}Overview of Loop Neurons}
\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__schematics.pdf}
\captionof{figure}{\label{fig:schematic__circuits}Two schematics of loop neurons. (a) Schematic of a point loop neuron (no dendritic tree) \cite{sh2018,sh2019} showing excitatory ($\mathsf{S_e}$) and inhibitory ($\mathsf{S_i}$) synapses, as well as synaptic weight update circuits ($\mathsf{W}$). The wavy, colored arrows are photons, and the black arrows are electrical signals. The synapses receive signals as faint as a single photon and add supercurrent to an integration loop. Upon reaching threshold, a signal is sent to the transmitter circuit ($\mathsf{T}$), which produces a pulse of photons that are sent to downstream synaptic connections. Electrical signals generated by synapses and the neuron cell body are used for synaptic weight updates. A refractory dendrite ($\mathsf{D_r}$) receives spike outputs from the neuron and provides feedback to the neuron to accomplish adaptive refraction. (b) A loop neuron with an elaborate dendritic tree \cite{sh2020}. The complex structure consists of excitatory and inhibitory synapses that feed into dendrites ($\mathsf{D}$). Each dendrite performs computations on the inputs and communicates the result to other dendrites for further processing or on to the cell body of the neuron ($\mathsf{N}$). The neuron itself acts as the final thresholding stage, and when its threshold is reached, light is produced by the transmitter ($\mathsf{T}$), which is routed to downstream synaptic connections.}
\end{figure}

Research in superconducting optoelectronic networks of loop neurons aspires to realize artificial neural systems with scale and complexity comparable to the human brain. We have introduced the concepts of loop neurons in a number of papers \cite{sh2018,sh2019,sh2020}, and we have demonstrated many of the principles experimentally \cite{buch2017,chbu2017,chbu2018,mcve2019}. Schematic diagrams of two types of loop neuron are shown in Fig.\,\ref{fig:schematic}. Figure \ref{fig:schematic}(a) shows the point-neuron concept \cite{sh2018,sh2019_jap}, while Fig.\,\ref{fig:schematic}(b) shows the concept when a dendritic tree is included \cite{sh2019_jstqe}. In these neurons, integration, synaptic plasticity, and dendritic processing are implemented with inductively coupled loops of supercurrent. It is due to the prominent role of superconducting storage loops that we refer to devices of this type as loop neurons. 

Operation of loop neurons is as follows. Photons from upstream neurons are received by a superconducting single-photon detector (SPD) at each synapse. Using a Josephson junction (JJ) in parallel with an SPD, synaptic detection events are converted into an integrated supercurrent which is stored in a superconducting loop. The circuit diagram of a synapse is shown in Fig.\,\ref{fig:circuits}(a). The amount of current that gets added to the integration loop during a photon detection event is determined by the synaptic weight. The synaptic weight is dynamically adjusted by another circuit combining SPDs and JJs, and all involved circuits are analog. When the integrated current of a given neuron reaches a (dynamically variable) threshold, an amplification cascade begins, resulting in the production of light from a waveguide-integrated semiconductor light emitter. The photons thus produced fan out through a network of dielectric waveguides and arrive at the synaptic terminals of other neurons where the process repeats.

In loop neurons, a synapse consists of an SPD in parallel with a JJ (which together transduce photons to supercurrent), and a superconducting loop, which stores a current proportional to the number of detected photon arrival events. This loop is referred to as the synaptic integration (SI) loop. Within each neuron, the loops of many synapses are inductively coupled to a larger superconducting loop, referred to as the neuronal receiving (NR) loop, thereby inducing an integrated current proportional to a weighted sum of the currents in all the neuron's synapses. When the current in this NR loop reaches a threshold, the neuron produces a current pulse in the form of one or more flux quanta. This current is amplified and converted to voltage to produce photons from a semiconductor $p-i-n$ junction.

The currents in the synaptic and neuronal loops are analogous to the membrane potential of biological neurons \cite{daab2001,geki2002}, and the states of flux in these loops are the principal dynamical variables of the synapses and neurons in the system. Inhibitory synapses can be achieved through mutual inductors with the opposite sign of coupling. Dendritic processing can be implemented straightforwardly by adding intermediate mutually inductively coupled loops between the synaptic and neuronal loops. Synapses can be grouped on dendritic loops capable of local, nonlinear processing and inhibition. Neurons with multiple levels of dendritic hierarchy can be implemented as multiple stages of integrating loops. The temporal scales of the SI and DI loops can be set with $L/r$ time constants. I expect all synaptic and dendritic signals to leak, enabling fading memory of recent activity. Also, I hypothesize that the ability to achieve a diversity of time constants through synapses and dendrites with many $L/r$ time constants will be advantageous, and I would like to investigate this in numerical studies.

Synaptic memory is also implemented based on the stored flux in a loop, referred to as the synaptic storage (SS) loop. The state of flux in the SS loop determines the current bias to the synaptic receiver circuit discussed above. This current bias is the synaptic weight. In contrast to the SI and DI loops, the SS loop is intended to store long-term memory, and therefore I do not expect to add a resistance to this loop. If the SS loop is created with a superconducting wire of high inductance, the loop can hold many discrete states of flux, and therefore can implement many synaptic weights, if desired. Synapses with a pseudo-continuum of hundreds of stable synaptic levels between minimal and maximal saturation values are possible. Transitions between these levels can be induced based on the relative arrival times of photons from the pre-synaptic and post-synaptic neurons, thereby establishing a means for spike-timing-dependent plasticity with one photon required for each step of the memory-update process. Binary synapses are also possible, and just as we expect a diversity of $L/r$ time constants to be advantageous for tracking activity over time, we expect a diversity of synapses raging from binary to multistable to be advantageous for striking a balance between adaptability and long-term memory retention.

In the present article, we work with the component definitions introduced in Ref.\,\cite{sh2020} and reproduced here for clarity. A \textit{synapse} is a circuit that receives photons with a superconducting nanowire single-photon detector and transduces the signal to an electrical current circulating in a storage loop. Specifically, Fig.\,\ref{fig:circuits}(a) is the circuit diagram of a synapse. A \textit{dendrite} is a circuit that receives as input a signal proportional to the electrical output of one or more synapses and/or dendrites, performs a transfer function on the sum of the inputs, and produces an electrical current circulating in a storage loop as the output. Specifically, Fig.\,\ref{fig:circuits}(b) is the circuit diagram of a dendrite. A \textit{neuron cell body} receives as input a signal proportional to the electrical output of one or more synapses and/or dendrites, performs a threshold operation on the sum of the inputs, and produces as output a pulse of photons if the threshold is exceeded. Following the production of a pulse, the neuron cell body experiences a refractory period wherein threshold is temporarily significantly elevated, making subsequent pulses temporarily unlikely or impossible. The circuits that accomplish the thresholding and electrical-to-optical transduction are discussed in Refs.\onlinecite{sh2018} and \onlinecite{sh2018_full}. Based on this defintion, the components $\mathsf{N}$ and $\mathsf{T}$ in the schematic of Fig.\,\ref{fig:schematic} comprise the cell body, although it is more biologically accurate to associate the transmitter $\mathsf{T}$ in this hardware with the axon hillock of a biological neuron. 

A neuron cell body can behave similarly to a dendrite in that both can perform a nonlinear threshold function on their inputs, but in the context of the hardware under consideration we make the distinction that a dendrite produces an electrical output that is to be communicated locally, while a neuron cell body produces an optical output that is to be communicated to synapses that may be spatially distant. Additionally, dendrites are envisioned to be diverse, with different dendrites performing different transfer functions, while all neuron cell bodies are envisioned to perform only the thresholding function leading to spike production. Further, the outputs from dendrites are functions with analog amplitude and a continuous temporal envelope, while the outputs from neuron cell bodies are stereotypical spike events wherein the amplitude is intended to be constant across spikes, and the temporal envelope is intended to approximate a delta function. The amplitude of the output from a neuron cell body carries no information, and all information output from the neuron cell body is encoded in the timing of the spikes. 

The term \textit{dendritic tree} refers collectively to all the synapses and dendrites that feed into a neuron cell body. Figure \ref{fig:schematic} is intended to illustrate the potential complexity and diversity of the dendritic tree. The output optical signals from a neuron cell body reach downstream synapses through a network of dielectric waveguides, optical fibers, and free-space interconnects, as described in Refs.\,\onlinecite{sh2018_full} and \onlinecite{chbu2017,chbu2018,sh2019_ne}. These optical paths are collectively referred to as the \textit{axonal tree} and are not shown in the schematic diagram. We define a \textit{neuron} to be a system comprising a dendritic tree, a neuron cell body, and an axonal tree.

A primary goal of developing the phenomenological model presented here is to elucidate similarities and differences between loop neurons and well-studied leaky integrate-and-fire (LIF) neurons. 

%make sure to talk about speeds (max firing speed, limits, leak rates briefly mentioned)
%mention notation, superscripts refer to loops, subscripts further description or indices

\section{\label{sec:leaky_integrators}Contrasting Loop Neurons with Leaky Integrate-and-Fire Point Neurons}
\begin{equation}
\label{eq:leaky_integrator}
\frac{dQ(t)}{dt} = f(t)-\frac{Q(t)}{\tau},
\end{equation}
The general form of a leaky integrator differential equation is given in Eq.\,\ref{eq:leaky_integrator}. Equation \ref{eq:leaky_integrator} models many physical systems wherein a quantity of interest, $Q$, accumulates in time due to a driving function, $f(t)$, while leaking with an exponential temporal dependence and time constant $\tau$. In neurons, the quantity of interest is the membrane potential, $V(t)$, which is due to charge on the membrane capacitance. This charge leaks exponentially due to shunt conductance between the surfaces of the membrane. The charge is provided by synapse events, which play the role of the driving function. In a point neuron model, processing in the dendritic tree is ignored, and it is assumed that the signal from all synapses is summed on the membrane, one can calculate the membrane potential on neuron $i$ with $Q(t)\rightarrow V_i(t)$ and
\begin{equation}
\label{eq:leaky_integrator__synapse_signal}
f(t) = q_0 \sum_j w_{ij} \sum_p \delta(t-t_p^{(j)}),
\end{equation}
where $w_{ij}$ is the synaptic weight, or efficacy, between neuron $j$ and neuron $i$, and $t_p^{(j)}$ is the time of the $p$th spike generated by neuron $j$.

In addition to leaky integration, a point-neuron model must produce spiking behavior and refraction. We arrive at a LIF model by supplementing Eqs.\,\ref{eq:leaky_integrator} and \ref{eq:leaky_integrator__synapse_signal} with the rule that when $V_i(t)$ reaches a threshold $V_i^{\mathrm{th}}$ from below, the neuron produces a spike, and the membrane potential is set to zero (or an appropriate reset value). While this LIF point neuron fails to capture much of the computational repertoire of biological neurons \cite{kose2000}, we recapitulate the assumptions of the model as a point of comparison for the loop neurons that are the focus of this work.

There are three distinctions to be made between the LIF model and a full loop neuron. First, loop neurons with thousands of connections are likely to employ a hierarchical dendritic tree, much like biological neurons \cite{me1994,loha2005,haah2016}, and tailored to the computational roles. Synapses store a record of their recent activity as flux in an inductive storage loop, and this signal is communicated to dendrites through transformers. Dendrites perform nonlinear operations on these signals and produce outputs as flux coupled to the next layer of the intra-neuron information-processing hierarchy. So whereas the quantity of interest in the LIF model is the membrane potential of the neuron cell body, in loop neurons the quantities of interest are the flux stored in all the synaptic and dendrite flux storage loops (the SI and DI loops discussed above).

Because the information in a loop neuron is contained in the flux storage loops the system need not be characterized by a single time constant of temporal decay. This is the second distinction between the LIF model and loop neurons. The LIF model is characterized by a single time constant determined by the rate at which charge leaks from the membrane capacitance, so a LIF neuron integrates information over a single temporal scale. By contrast, each synapse and dendrite of a loop neuron can have a distinct time constant (set by the $L/r$ time constant of the loop), so the synapto-dendritic tree of a loop neuron can integrate information across many orders of magnitude in temporal scale.

The third distinction between the LIF model and loop neurons relates to reset after production of a spike. When a LIF neuron produces a spike, its membrane potential is forced to a reset value, and all information accumulated by the neuron is discarded. By contrast, no such reset occurs in loop neurons. The cell body of the neuron receives signals from the final layer of dendrites as well as direct synapses. When the net flux from these signals drives a Josephson junction in the cell body above threshold, it produces an action potential that sends light to downstream synaptic connections. Refraction is accomplished when the signal that starts this action potential also triggers a specific dendrite that couples an inhibitory signal back to the cell body, achieving brief, negative self-feedback, which enables pulsatile behavior. Because the state of flux in the SI and DI loops is not affected by the neuron reaching threshold (with the exception of the refractory dendrite), the information contained in those loops is maintained through the firing process. 

As the subsequent sections will describe, within the framework of loop neurons, synapses behave as leaky integrators that receive delta-function temporal inputs and cumulative outputs with exponential temporal decay of nearly arbitrary time constant. Dendrites are leaky integrators as well, obeying a nearly identical differential equation to synapses. Neurons are modeled simply as a dendrite with self-feedback accomplishing refraction through another dendrite. An entire loop neuron is a network of coupled leaky integrators, the output of each given as the input to another. Communication between neurons is by optical pulse events. Thus, while a LIF point-neuron model describes a single integrator with a single leak rate, a loop neuron is itself an elaborate network of coupled integrators accumulating and leaking information across temporal scales. It is premature to conclude that a network of such complex neurons is superior to simpler LIF point neurons for information processing. The objective of this paper is to develop the mathematical model that will enable efficient simulation of large networks of loop neurons so their utility can be assessed.

\section{\label{sec:synapses}Synapses as Leaky Integrators}
\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig__synapses__circuits__responses.pdf}
\captionof{figure}{\label{fig:synapses__circuits__responses}Caption of fig:synapses circuits responses.}
\end{figure*}
The objective of this section is to express the output of a synapse as a function of the input without solving the explicit circuit equations. We would like to capture the behavior of each synapse with a single ODE, rather than a system of coupled ODEs which is required to exactly model the circuit. We would also like to enable the ODE to be stepped through in time with as large a time step as possible, rather than requiring simulation on the picosecond time scale as required to accurately capture the dynamics of the Josephson junction.

The synapse under consideration in shown in Fig.\,\ref{fig:synapses__circuits__responses}(a). In this simple form, the circuit comprises a superconducting-nanowire single-photon detector (SPD) in parallel with a Josephson junction (JJ). The JJ is embedded in a flux storage loop, referred to as the synaptic integration (SI) loop. We will also discuss a slightly more complex synapse circuit with three JJs, as shown in Fig.\,\ref{fig:synapses__circuits__responses}(b). The inputs to the circuit are photonic pulses generated by other neurons during firing events, and the output of the circuit is due to the integrated current circulating in the SI loop. This current couples flux through the mutual inductor ($M^{\mathrm{sy}}$) to a dendritic receiving (DR) loop or to the neuronal receiving (NR) loop of the cell body itself. The current bias $I^{\mathrm{sy}}$ is also an input to the circuit. This bias determines how much current is added to the SI loop with each synapse event. It is assumed that $I^{\mathrm{sy}}$ is slowly varying on the time scale of inter-spike intervals due to synaptic plasticity effects. The current bias to the SPD is assumed to be fixed for all time (at $20\,$\textmu A in this work). The critical current of all JJs considered here is $40\,$\textmu A.

The model depends on two factors. First, current will be added to the SI loop at a rate that depends only on the instantaneous current passing through the synaptic firing junction ($I_{\mathrm{sf}}$ through $J_{\mathrm{sf}}$). Second, the current that has been added to the SI loop will decay exponentially with time constant $\tau^{\mathrm{si}} = L^{\mathrm{si}}/r^{\mathrm{si}}$. These two factors allow us to model the temporal rate of change of current in the SI loop with a leaky integrator equation:
\begin{equation}
\label{eq:synapses__leaky_integrator}
\frac{dI^{\mathrm{si}}(t)}{dt} = I_{\mathrm{fq}} \, R_{\mathrm{fq}}\left(I^{\mathrm{drive}}(t),I^{\mathrm{si}}(t)\right) -\frac{I^{\mathrm{si}}(t)}{\tau^{\mathrm{si}}}.
\end{equation}
The function $R^{\mathrm{fq}}\left(I^{\mathrm{drive}},I^{\mathrm{si}}\right)$ represents the rate at which fluxons are added to the SI loop as a function of the total current across the synaptic firing junction, $I^{\mathrm{sf}}$, as well the current circulating in the SI loop, $I^{\mathrm{si}}$. $R^{\mathrm{fq}}$ has dimensions of fluxons per second. $I_{\mathrm{fq}} = \Phi_0/L^{\mathrm{si}}$ is the current added to the SI loop with each fluxon (described in more detail below), and has dimension of amperes per fluxon. The flux-induced current $I^{\mathrm{si}}$ circulates in the SI loop in a manner that counters the bias to $J^{\mathrm{si}}$. This counter-biasing that occurs as current accumulates in the SI loop is a form of self-feedback, which has the effect of limiting the total current that can be added to the SI loop. This saturation effect is a nonlinearity that leads to saturation in the synaptic response in a manner that is functionally similar to short-term synaptic plasticity \cite{sh2020}.

The synapse model requires specification of the form of the function $R_{\mathrm{fq}}\left(I^{\mathrm{drive}}(t),I^{\mathrm{si}}(t)\right)$. There are several methods to obtain this function, and for the reasons explained in Appendix\,\ref{apx:synapses_model}, we employ a purely numerical technique based on circuit simulations. In brief, a series of time-domain circuit simulations are conducted to obtain a lookup table for $R_{\mathrm{fq}}$ at discrete values of $I^{\mathrm{drive}}$ and $I^{\mathrm{si}}$. These rate arrays are shown in Fig.\,\ref{fig:synapses__circuits__responses}(c) and (d). Similarly, the response of the SPD following a synapse event is simulated to determine the temporal response of $I^{\mathrm{spd}}$, which enters $I^{\mathrm{drive}}$ and therefore the synaptic rate array. SPD responses for the one- and three-junction synapses are shown in Appendix\,\ref{apx:synapses_model}. A small number of circuit simulations is required to construct these lookup tables, and they are applicable to arbitrary values of $L^{\mathrm{si}}$ and $r^{\mathrm{si}}$ as well as the entire dynamic range of $I^{\mathrm{sy}}$. These arrays are accessed when stepping through Eq.\,\ref{eq:synapses__leaky_integrator} in time, thereby replacing the differential equations that would otherwise be required, and enabling utilization of temporal resolution on the order of 100\,ps - 1\,ns when simulating the synaptic response rather than the 1\,ps - 10\,ps time step commonly used when simulating JJs.

(cmpr)


To quantify the accuracy, we use a $\chi^2$ of the form
\begin{equation}
\label{eq:chi_squared}
\chi^2 = \frac{\int dt \left|I^{\mathrm{si}}_{\mathrm{spice}}(t) - I^{\mathrm{si}}_{\mathrm{ode}}(t)\right|^2}{\int dt \left|I^{\mathrm{si}}_{\mathrm{spice}}(t)\right|^2}.
\end{equation}


\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig__synapses__comparison__1jj__main.pdf}
\captionof{figure}{\label{fig:synapses__comparison__1jj__main}Caption of fig:synapses comparison 1jj main.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig__synapses__comparison__3jj__main.pdf}
\captionof{figure}{\label{fig:synapses__comparison__3jj__main}Caption of fig:synapses comparison 3jj main.}
\end{figure*}

The rate arrays presented in Fig.\,\ref{fig:synapses__circuits__responses}(c) and (d) elucidate the response of the circuits. One can see that for a given value of $I^{\mathrm{drive}}$ the rate begins high and gradually decays to zero as a function of $I^{\mathrm{si}}$, with an abrupt cessation at a saturation value $I^{\mathrm{si}}_{\mathrm{sat}}$. The important distinction between the one-junction synapse and the three-junction synapse is that $I^{\mathrm{si}}_{\mathrm{sat}}$ is a strong function of $I^{\mathrm{drive}}$ in the one-junction case, while $I^{\mathrm{si}}_{\mathrm{sat}}$ is nearly independent of $I^{\mathrm{drive}}$ in the three-junction circuit. This feature allows the total signal that can be accumulated in the SI loop to be nearly decoupled from the synaptic bias current, $I^{\mathrm{sy}}$. In both cases, the number of fluxons produced in a synapse event is determined by the operating point in the rate array at the moment the synapse event occurs. Assuming fixed bias to the SPD, this rate of fluxon addition to the SI loop is uniquely determined by the synaptic bias current, $I^{\mathrm{sy}}$, and the total integrated current in the SI loop, $I^{\mathrm{si}}$. For the one junction synapse, $I^{\mathrm{sy}}$ affects the synaptic efficacy by determining the rate during a synapse event and by setting the saturation point, whereas in the three-junction synapse the saturation point is nearly independent of the synaptic bias current. In this case, the saturation current in the SI loop, $I^{\mathrm{si}}_{\mathrm{sat}}$, is primarily affected by the the saturation control bias current, $I^{\mathrm{sc}}$. With these two degrees of freedom decoupled, the number of fluxons generated in a synapse event is determined primarily by $I^{\mathrm{sy}}$, while the maximum signal that can be integrated at the synapse is primarily determined by $I^{\mathrm{sc}}$. In the three-junction case, a large number of weak synapse events can lead to as large of a signal as a small number of strong synapse events. This is not the case on the one-junction synapse. The computational significance of this circuit feature remains to be assessed. 

The synapse behaves as a leaky integrator, and the rate function provides the driving term. Yet the total rate at which current is added to the SI loop is scaled by the current per fluxon, $I_{\mathrm{fq}}$, which scales inversely with the loop inductance, a parameter fixed in fabrication. With $I_{\mathrm{fq}}$ large, the saturation current, $I^{\mathrm{si}}_{\mathrm{sat}}$, at which the fluxon generation rate drops to zero, will be reached with few fluxons. Inductances can be achieved to give saturation with as few as one fluxon or as many as tens of thousands. Synapses can thus be binary or analog with high bit depth. The coarse operating point is set in hardware with the choice of SI loop inductance, and it is dynamically adjusted through plasticity circuits acting to modify $I^{\mathrm{sy}}$, and therefore the number of fluxons generated in a synapse event. 

The inductance also affects the leak rate, set by $\tau^{\mathrm{si}}$ in Eq.\,\ref{eq:synapses__leaky_integrator}. Because the synaptic time constant is set by $L^{\mathrm{si}}/r^{\mathrm{si}}$, the synapse storage capacity and leak rate can be adjusted independently. Arbitrarily fast leak rates are straightforward to achieve, and long time constants are also attainable in such passive superconducting circuits. Typical values of $L^{\mathrm{si}}$ are likely to be in the range 10\,nH - 10\,\textmu H. Such large inductances are possible due to high-kinetic-inductance materials \cite{}. Resistors of 1\,m$\Omega$ require care in fabrication, but are entirely possible, making passive leak time constants of 10\,ms, and potentially longer. Such time constants may make possible dynamics on biological timescales as well as the 50\,ns time scale observed when loop neurons burst at their maximum rate. The dissipationless persistence of supercurrents due to flux stored in loops also enables long-term memory retention and plasticity of various bias currents to enable learning and homeostatic adaptation. In the leaky integrator synapse model, the time constant $\tau^{\mathrm{si}}$ determines the time scale over which information is integrated. The ability to construct systems wherein neurons can draw input from synapses with such a diversity of time constants provides an opportunity to study the role of this factor in temporal dynamics such as oscillations, synchronization, and scale-free neuronal avalanches.

The leaky-integrator synapse model of Eq.\,\ref{eq:synapses__leaky_integrator} captures the functionality of the circuit with acceptable accuracy, and it provides a means to understand the operation conceptually. The interesting dynamics are captured in the rate array, which shows how the excitation due to synapse events and the synaptic bias current drives the synapse, while the integrated signal provides saturation through self-feedback. The flux resolution of the integration loop is represented by the prefactor $I_{\mathrm{fq}}$, while the integration time is set by $\tau^{\mathrm{si}}$. The available parameter space is large, and a primary goal of developing this model is to enable simulations that will determine if the resulting functionality space is also useful.

In addition to these degrees of freedom, one further factor determines the magnitude and sign of the drive delivered from a synapse to a dendrite or neuron. This factor is the mutual inductance between the SI loop and the DR or NR loop. We now discuss how the current in the SI loop couples through mutual inductance to become a flux drive coupled into a receiving loop. 

%I^{\mathrm{drive}} -> I^{\mathrm{ex}}, the excitation current?

%From this perspective, we can disambiguate several factors that contribute to the total drive from a synapse to a dendrite or neuron. First, with a fixed SPD bias current, the number of fluxons generated during a synapse event is determined by the synaptic bias current, $I^{\mathrm{sy}}$ , which can adapt dynamically in response to activity. Second, the total amount of current added to the SI loop
 


%It is this counter-biasing that causes the SI loop to saturate as a function of $I^{\mathrm{si}}$. When $I^{\mathrm{si}}$ becomes sufficiently large, the fluxon generated by $J^{\mathrm{jtl}}$ does not provide enough current to drive $J^{\mathrm{si}}$ above $I_c$, and the stored flux in the SI loop must decay before subsequent synapse events can add to the integrated signal.




%We begin with the hypothesis that the circuit can be modeled as a leaky integrator, with the primary dynamical quantity being the integrated current circulating in the SI loop, and a drive term resulting from photon detection events. When the SPD receives a photon, its bias current, $I_{\mathrm{spd}}$, is diverted to the synaptic firing junction, $J_{\mathrm{sf}}$. If the combination $I_{\mathrm{spd}}+I_{\mathrm{sy}}$ exceeds the critical current of $J_{\mathrm{sf}}$, a series of fluxons will be generated. At the circuit level, $J_{\mathrm{sf}}$ will first produce a fluxon. The current from this fluxon will drive $J_{\mathrm{jtl}}$ above $I_c$, causing it to produce a fluxon. Likewise, this fluxon will drive $J_{\mathrm{si}}$ above $I_c$, and the fluxon thus produced will be added to the SI loop, increasing the value of the dynamical variable of interest. After this fluxon has increased $I_{\mathrm{si}}$, if $J_{\mathrm{sf}}$ is still being held above $I_c$ by the combination of $I_{\mathrm{spd}}$ and $I_{\mathrm{sy}}$, another fluxon will be produced and propagate through the circuit in the same manner. Depending on the strength of the synaptic weight and the flux storage capacity of the SI loop, a single synapse event may add anywhere from a few fluxons up to over one thousand fluxons to the SI loop. Each fluxon adds $\Delta I_{\mathrm{si}} = \Phi_0/L_{\mathrm{si}}$ to the circulating current in the SI loop. $\Phi_0 = h/2e \approx 2\times10^{-15}\mathrm{V}\cdot\mathrm{s}$ is the magnetic flux quantum. The current in the SI loop decays with time constant $\tau_{\mathrm{si}} = L_{\mathrm{si}}/r_{\mathrm{si}}$. Given this picture of circuit operation, we can see a path toward modeling the synapse as a leaky integrator. The driving term is determined by the rate of fluxon production, and the leak term is governed by $\tau_{\mathrm{si}}$.

%\cite{amfu1994,fudr2005,fuab2007}

\section{\label{sec:dendrites}Dendrites as Leaky Integrators}
In the world of loop neurons, a dendrite is much like a synapse, except the input current from an SPD is replaced by input flux from an SI loop, and instead of a single input, many are possible. Dendrite circuits are shown in Fig.\,\ref{fig:dendrites__circuits__responses} (a) and (b). Like the case of synapses, we consider a simple implementation where the firing junction is embedded in the flux-integration output loop as well as an extended version where the receiving loop and storage loop are separated.

\begin{equation}
\label{eq:dendrites__leaky_integrator}
\frac{dI^{\mathrm{di}}(t)}{dt} = R_{\mathrm{fq}} \left( \Phi^{\mathrm{dr}}_a(t),I^{\mathrm{di}}(t) \right)\,I_{\mathrm{fq}} - \frac{I^{\mathrm{di}}(t)}{\tau^{\mathrm{di}}}.
\end{equation}
$R_{\mathrm{fq}} \left( \Phi^{\mathrm{dr}}_a(t),I^{\mathrm{di}}(t) \right)$ is the rate function, and $\Phi^{\mathrm{dr}}_a(t)$ is the total applied flux to the dendritic receiving loop:
\begin{equation}
\label{eq:dendrites__applied_flux}
\Phi^{\mathrm{dr}}_a(t) = \sum_i M_i I_i^{\mathrm{si/di}}(t).
\end{equation}
The sum runs over all inputs to the dendritic receiving loop, and the superscript notation $I_i^{\mathrm{si/di}}$ indicates that inputs may be from synaptic or dendritic integration loops.

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig__dendrites__circuits__responses.pdf}
\captionof{figure}{\label{fig:dendrites__circuits__responses}Caption of fig:dendrites responses.}
\end{figure*}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__comparison__2jj__lin_ramp.pdf}
\captionof{figure}{\label{fig:dendrites__comparison__2jj__lin_ramp}Caption of fig:dendrites comparison 2jj lin ramp.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__comparison__2jj__exp_pls_seq.pdf}
\captionof{figure}{\label{fig:dendrites__comparison__2jj__exp_pls_seq}Caption of fig:dendrites comparison exp pls seq.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__comparison__2jj__sq_pls_seq.pdf}
\captionof{figure}{\label{fig:dendrites__comparison__2jj__sq_pls_seq}Caption of fig:dendrites comparison sq pls seq.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__comparison__4jj__lin_ramp.pdf}
\captionof{figure}{\label{fig:dendrites__comparison__4jj__lin_ramp}Caption of fig:dendrites comparison 4jj lin ramp.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__comparison__4jj__sq_pls_seq.pdf}
\captionof{figure}{\label{fig:dendrites__comparison__4jj__sq_pls_seq}Caption of fig:dendrites comparison 4jj sq pls seq.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__comparison__4jj__exp_pls_seq.pdf}
\captionof{figure}{\label{fig:dendrites__comparison__4jj__exp_pls_seq}Caption of fig:dendrites comparison 4jj exp pls seq.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__error_vs_dt__2jj.pdf}
\captionof{figure}{\label{fig:dendrites__error_vs_dt__2jj}Caption of fig:dendrites 2jj error vs dt.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__error_vs_dt__4jj.pdf}
\captionof{figure}{\label{fig:dendrites__error_vs_dt__4jj}Caption of fig:dendrites 4jj error vs dt.}
\end{figure}

\section{\label{sec:neurons}Loop Neurons as Coupled Leaky Integrators}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__point_neuron__one_synapse__schematic__circuit.pdf}
\captionof{figure}{\label{fig:point_neuron__one_synapse__schematic__circuit}Caption of fig:point neuron  one synapse  schematic  circuit.}
\end{figure}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig__point_neuron__one_synapse__data.pdf}
\captionof{figure}{\label{fig:point_neuron__one_synapse__data}Caption of fig:point neuron  one synapse  data.}
\end{figure*}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__point_neuron__nine_synapses__schematic.pdf}
\captionof{figure}{\label{fig:point_neuron__nine_synapses__schematic__circuit}Caption of fig:point neuron  nine  synapses  schematic.}
\end{figure}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig__point_neuron__nine_synapses__data.pdf}
\captionof{figure}{\label{fig:point_neuron__nine_synapses__data}Caption of fig:point neuron  nine synapses  data.}
\end{figure*}

\section{\label{sec:discussion}Discussion}

\vspace{1em}
\section{\label{sec:acknowledgements}Acknowledgements}
\noindent This is a contribution of NIST, an agency of the US government, not subject to copyright.

\appendix

\section{\label{apx:synapses_model}Further details regarding synapse model}

\subsection{Rate of production of fluxons by a Josephson junction}
Throughout this work, the rate function, $R_{\mathrm{fq}}$, has been obtained numerically from circuit simulations. However, it is worth considering the origin of this term with closed-form expression to the extent possible. In this subsection we present an analytical model of the one-junction synapse and explain why the model is not extensible to synapses with additional junctions or to dendrites.

This rate term in the leaky-integrator description of a superconducting optoelectronic synapse ($R_{\mathrm{fq}}$ in Eq.\,\ref{eq:synapses__leaky_integrator}) originates from well-known properties of Josephson junctions within the formalism of the resistively and capacitively shunted junction (RCSJ) model \cite{vatu1998,ka1999,ti1996}. A JJ will produce a discrete voltage pulse, referred to as a fluxon, after a time $t_{\mathrm{fq}}$ determined by the relation
\begin{equation}
\label{eq:jj__fluxon_production}
\int_0^{t_{\mathrm{fq}}} V(t) \, dt = \Phi_0.
\end{equation}
If $V(t)$ is slowly varying on the scale of $t^{\mathrm{fq}}$, we can make the approximation $V(t)\,t^{\mathrm{fq}} \approx \Phi_0$, giving the rate of fluxon generation as
\begin{equation}
\label{eq:jj__fluxon_rate}
R_{\mathrm{fq}}(t) =  \frac{1}{t_{\mathrm{fq}}} = \frac{V(t)}{\Phi_0}.
\end{equation}

An expression for $R_{\mathrm{fq}}$ can be obtained if the voltage across the synaptic firing junction is known. In the case of the one-JJ synapse, this voltage is approximately given by
\begin{equation}
\label{eq:jj__current_voltage}
V^{\mathrm{sf}}(I^{\mathrm{sf}}) = \begin{cases} V_0\left[ \left( \frac{I^{\mathrm{sf}}}{I_c-I_r} \right)^{\mu_1} - 1 \right]^{\mu_2}, & \text{for } I > (I_c-I_r)_+ \\
0, & \text{for } I < (I_c)_-
\end{cases}
\end{equation} 
The notation $I > (I_c-I_r)_+$ refers to currents $I$ that have exceeded $I_c$, driving the junction into the resistive state, and have yet to drop below $I_c-I_r$, and are therefore demonstrating hysteresis. Likewise, $I < (I_c)_-$ refers to currents that are below $I_c$ while the junction is not in the hysteretic state. $I_r = 1.1768$\,\textmu A is the reset current associated with hysteresis on $J_{\mathrm{sf}}$ due to the fact that $\beta_c = 0.95 > 0$. The values $\mu_1$, $\mu_2$, and $V_0$ depend on the specific parameters of the shunt resistance and capacitance in the RCSJ model. For the junctions considered here, fits to WRSpice data give $\mu_1 = 3.464271$, $\mu_2 = 0.306768$, and $V_0 = 233.966$\,\textmu V. From Eqs.\,\ref{eq:jj__fluxon_production} and \ref{eq:jj__current_voltage} we can obtain $R^{\mathrm{fq}}$. In the case where $I^{(\mathrm{sf})}$ is slowly varying compared to $t^{\mathrm{fq}}$, $R^{\mathrm{fq}} = V^{(\mathrm{sf})}/\Phi_0$. The voltage given by Eq.\,\ref{eq:jj__current_voltage} is only meaningful as a time-averaged approximation, and it ignores the phase of the present across the junction. 

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig_apx__synapses__1jj_circuit.pdf}
\captionof{figure}{\label{fig:apx__synapses__1jj_circuit}Caption of fig:apx synapses 1jj circuit. Need to put all correct labels in this figure (see pg 11 of green lab notebook)}
\end{figure}
By using the explicit form for the voltage on the synaptic firing junction, we can obtain a pair of coupled ODEs that determine the time evolution of the circuit. A labeled diagram of the circuit under consideration is shown in Fig.\,\ref{fig:apx__synapses__1jj_circuit}. The coupled ODEs are:
\begin{equation}
\label{eq:syn_1jj_analytical_1}
\frac{d I^{\mathrm{si}}}{dt} = \frac{V^{\mathrm{sf}}( I^{\mathrm{sf}} - I^{\mathrm{si}} )}{L^{\mathrm{si}}} - \frac{r^{\mathrm{si}}}{L^{\mathrm{si}}} I^{\mathrm{si}},
\end{equation}
and
\begin{equation}
\label{eq:syn_1jj_analytical_2}
\begin{split}
\frac{d I^{\mathrm{sf}}}{dt} & = \frac{r^{\mathrm{spd1}}}{L^{\mathrm{spd}}} \left( I^{\mathrm{spd}}+^{\mathrm{sy}}-I^{\mathrm{sf}} \right) \\
& -\frac{r^{\mathrm{spd2}}}{L^{\mathrm{spd}}} \left( I^{\mathrm{sf}}-I^{\mathrm{sy}} \right) \\
& -\frac{1}{L^{\mathrm{spd}}} V^{\mathrm{sf}}(I^{\mathrm{sf}}-I^{\mathrm{si}}).
\end{split}
\end{equation}
Note that Eq.\,\ref{eq:syn_1jj_analytical_1} is a leaky integrator ODE for the current in the SI loop, just as Eq.\,\ref{eq:synapses__leaky_integrator} in the main text. Here it is coupled to Eq.\,\ref{eq:syn_1jj_analytical_2}, which captures the current through the synaptic firing junction due to the response of the SPD as well as the current circulating in the SI loop. By stepping through Eqs.\,\ref{eq:syn_1jj_analytical_1} and \ref{eq:syn_1jj_analytical_2}, satisfactory agreement is found with SPICE circuit simulations that do not ignore the phase across the Josephson junction. However, to obtain agreement, a short time step on the order of a picosecond is required. The goals of the synaptic model are to reduce the system to a single ODE that can be stepped on a longer time scale. Simulating the model of Eqs.\,\ref{eq:syn_1jj_analytical_1} and \ref{eq:syn_1jj_analytical_2} on a 1\,ps time grid accomplishes neither of these goals. 

In addition to these shortcomings of the analytical approach to a synapse model, the time-averaged approximation for the voltage across a junction (Eq.\,\ref{eq:jj__current_voltage}) provides a poor model for any circuit wherein two JJs interact with each other. Aside from the one-junction synapse, all other circuits considered in this work rely on multiple coupled JJs. In such circuits, the phases across the junctions cannot be ignored. For these reasons, we resort to numerical means of obtaining the rate functions ($R_{fq}$) for synapses and dendrites, as discussed in the following subsection.

\subsection{Determining the form of the rate function and detector with circuit simulations}
the rate at which current is added to the SI loop depends on the total current biasing the junction ($I_{\mathrm{sf}}$), which depends on the drive ($I_{\mathrm{drive}} = I_{\mathrm{spd}}+I_{\mathrm{sy}}$) and the integrated current in the SI loop ($I_{\mathrm{si}}$). We therefore wish to form an array with values of $R_{\mathrm{fq}}$ corresponding to points in the $I_{\mathrm{sf}}$ - $I_{\mathrm{drive}}$ plane, as shown in Fig.\,\ref{fig:synapses__circuits__responses}(c). To obtain such an array, we replace the SPD and the resistor $r^{\mathrm{spd}}$ with a current source. We choose a large value for $L^{\mathrm{si}}$ so that many fluxons are required to saturate the SI loop, and we let $r^{\mathrm{si}} \rightarrow 0$ so the loop has no leak. We then simulate the circuit in time. For a given value of $I^{\mathrm{drive}}$, one obtains a list of values of $R_{\mathrm{fq}}$ as a nearly continuous function of $I^{\mathrm{si}}$ as the loop fills to saturation. With this approach, there are multiple methods of calculating the rate. One may find all voltage peaks corresponding to fluxons, and determine the time between them. Alternatively, one may track the phase across $J_{\mathrm{sf}}$ or the quantity $I^{\mathrm{si}}/I_{\mathrm{fq}}$, and differentiate with respect in time. If one wishes to calculate the rate based on fluxon times, high temporal resolution is necessary, and the output data files can become large. The rate arrays used in this work have been obtained by smoothing the quantity $I^{\mathrm{si}}/I_{\mathrm{fq}}$ with a SavitzkyGolay filter and then differentiating in time provides. By repeating this procedure for multiple values of $I^{\mathrm{drive}}$, one can build up a representation of $R_{\mathrm{fq}}(I^{\mathrm{drive}},I^{\mathrm{si}})$, as shown in Fig.\,\ref{fig:dendrites__circuits__responses}(c). In this work, we have used WRSpice \cite{wh1991} to perform the circuit simulations. The netlist was edited with Python scripts to facilitate looping over $I^{\mathrm{drive}}$.

A beneficial aspect of this method of determining the rate array is that it applies directly to the three-junction synapse as well. To obtain the rate array shown in Fig.\,\ref{fig:synapses__circuits__responses}(d), the same method of replacing the SPD with a static current source was used to construct each trace of $R_{\mathrm{fq}}$ versus $I^{\mathrmn{si}}$. We have used a resolution of 250\,nA in $I^{\mathrmn{drive}}$. Therefore, with this technique, roughly 40 circuit simulations are sufficient to obtain a rate array with sufficient resolution to provide the accuracy of synaptic responses presented here. Each circuit simulation captures 1\,\textmu s of simulated time, requiring $xy$\,s of CPU time. Once these simulations are run and the rate array has been constructed, it is used as a lookup table on each time step of the simulation of the synaptic ODE, Eq.\,\ref{eq:synapses__leaky_integrator}. A different lookup table must be constructed for the one-junction and three-junction circuits, but each table is applicable to a broad range of values of the circuit parameters $L^{\mathrm{si}}$, $r^{\mathrm{si}}$, and $I^{\mathrm{sy}}$. The table used in this work for the one-junction synapse is 536\,kB, while that for the three-junction synapse is 739\,kB.

One of the inputs to the rate array, $R_{\mathrm{fq}}$, is $I^{\mathrm{drive}}$, which is the sum of the synaptic bias current, $I^{\mathrm{sy}}$, and the current diverted from the SPD upon detection of a synapse event, $I^{\mathrm{spd}}$. When an SPD detects a photon, it develops a large resistive section ($\approx 5$\,k$\Omega$) for a brief duration ($\approx 200\,$\,ps) \cite{}. This resistance returns to zero once superconductivity is restored. Accurately modeling the temporal response of the current $I^{\mathrm{spd}}$ following a synapse event unfortunately requires knowledge of the voltage across $J^{\mathrm{sf}}$ on the picosecond time scale. To avoid adding this burden to our numerical scheme, we also resort to a lookup-table approach for $I^{\mathrm{spd}}$. Here we simulate the circuits of Fig.\,\ref{fig:synapses__circuits__responses}(a) and (b) with the SPD modeled as a 200\,ps, 5\,k$\Omega$ resistive segment following a detection event. We use a large SI inductance to ensure self-feedback is negligible, and we simulate the circuit for roughly 40 values of $I^{\mathrm{sy}}$ to obtain $I^{\mathrm{spd}}$ as a function of time following a synapse event. This lookup table is then used to determine the drive to $R_{\mathrm{fq}}$ following a synapse event. While this approach to determining the SPD response leads to acceptable accuracy and speed when used to simulate synapses, it is by far the least elegant aspect of the model.

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig_apx__synapses__spd__responses.pdf}
\captionof{figure}{\label{fig:synapses__spd_responses}Caption of fig:synapses spd responses.}
\end{figure}

\section{\label{apx:synapses_data}Further data regarding synapse modeling}


\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__comparison__1jj__tau_si.pdf}
\captionof{figure}{\label{fig:synapses__comparison__1jj__tau_si}Caption of fig:synapses comparison 1jj tau si.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__comparison__1jj__L_si.pdf}
\captionof{figure}{\label{fig:synapses__comparison__1jj__L_si}Caption of fig:synapses comparison 1jj L si.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__comparison__1jj__I_sy.pdf}
\captionof{figure}{\label{fig:synapses__comparison__1jj__I_sy}Caption of fig:synapses comparison 1jj I sy.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__error_vs_dt__1jj.pdf}
\captionof{figure}{\label{fig:synapses__error_vs_dt__1jj}Caption of fig:synapses error vs dt 1jj.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__comparison__3jj__tau_si.pdf}
\captionof{figure}{\label{fig:synapses__comparison__3jj__tau_si}Caption of fig:synapses comparison 3jj tau si.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__comparison__3jj__L_si.pdf}
\captionof{figure}{\label{fig:synapses__comparison__3jj__L_si}Caption of fig:synapses comparison 3jj L si.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__comparison__3jj__I_sy.pdf}
\captionof{figure}{\label{fig:synapses__comparison__3jj__I_sy}Caption of fig:synapses comparison 3jj I sy.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=17.2cm]{figures/_fig_apx__synapses__error_vs_dt__3jj.pdf}
\captionof{figure}{\label{fig:synapses__error_vs_dt__3jj}Caption of fig:synapses error vs dt 3jj.}
\end{figure*}

\section{\label{apx:dendrites}Further data regarding dendrite modeling}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__min_max__2jj.pdf}
\captionof{figure}{\label{fig:dendrites_min_max__2jj}Caption of fig:dendrites min max 2jj.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__dendrites__min_max__4jj.pdf}
\captionof{figure}{\label{fig:dendrites_min_max__4jj}Caption of fig:dendrites min max 4jj.}
\end{figure}

\section{\label{apx:fan_in}Analysis of dendritic and neuronal fan-in}

\begin{figure}[h!]
\includegraphics[width=8.6cm]{figures/_fig__fan-in__circuits.pdf}
\captionof{figure}{\label{fig:fan-in__circuits}Caption of fig:fan-in circuits.}
\end{figure}

We would like to know how to choose the inductors shown in Fig.\,\ref{fig:fan-in__circuits}(a) so the applied flux stays in the appropriate operating range as the number of synaptic connections, $N$, is modified. This consideration will identify the limit of how many synapses can be coupled to a single dendrite or neuron\textemdash the fan-in limit. 

We have seen that the operation of a dendrite (or neuron cell body) is significantly shaped by the inductance of the DR loop. We would therefore like to be able to increase the number of synaptic connections without changing the total inductance of this loop. This total inductance is given by
\begin{equation}
\label{eq:fan-in__direct_input__si_inductance}
L^{\mathrm{nr1}}_{\mathrm{tot}} = \sum_i L^{\mathrm{nr1}}_i = N L^{\mathrm{nr1}}_0
\end{equation}
We would like $L^{\mathrm{nr1}}_{\mathrm{tot}}$ to be independent of $N$, which is possible with $L^{\mathrm{nr1}}_N = L^{\mathrm{nr1}}_0/N$. The total applied flux to the NR loop is given by
\begin{equation}
\label{eq:fan-in__direct_input__applied_flux}
\Phi_{\mathrm{a}}^{\mathrm{nr}} = \sum_i M_i^{\mathrm{si|nr}} \, I_i^{\mathrm{si}}.
\end{equation}
Suppose all SI loops contain a given current, $I_0^{\mathrm{si}}$. We would like the applied flux to the NR loop, $\Phi_{\mathrm{a}}^{\mathrm{nr}}$, to be independent of $N$:
\begin{equation}
\label{eq:fan-in__direct_input__flux_independent_of_N}
\frac{ \Phi_{\mathrm{a}}^{\mathrm{nr}} }{ I_0^{\mathrm{si}} } = \mathrm{cnst.} \neq f(N).
\end{equation}
In the case where all mutual inductors between synapses and the NR loop are equal,
\begin{equation}
\label{eq:fan-in__direct_input__flux_independent_of_N_2}
\frac{ \Phi_{\mathrm{a}}^{\mathrm{nr}}(N) }{ I_0^{\mathrm{si}} } = N\,k\,\sqrt{ L_N^{\mathrm{si2}} \, L_N^{\mathrm{nr1}}},
\end{equation}
where $L_N^{\mathrm{si2}}$ and $L_N^{\mathrm{nr1}}$ are the value ofs $L^{\mathrm{si2}}$ and $L^{\mathrm{nr1}}$ that we should choose for the SI loop output inductor when there are $N$ synapses coupled directly to the NR loop. We have seen above that we need $L_N^{\mathrm{nr1}} = L_0^{\mathrm{nr1}}/N$ to maintain the inductance of the NR loop. Therefore, in order for Eq.\,\ref{eq:fan-in__direct_input__flux_independent_of_N_2} to satisfy Eq.\,\ref{eq:fan-in__direct_input__flux_independent_of_N}, we must also have $L_N^{\mathrm{si2}} = L_0^{\mathrm{si2}}/N$.

This consideration does not explicitly identify a fan-in limit. This limit arises due to practical considerations related to making $L^{\mathrm{si2}}$ and $L^{\mathrm{nr1}}$ exceedingly small as $N$ grows large. With $L_0 \approx 1$\,pH a rough lower limit on what we would like to fabricate, and $L_{\mathrm{tot}}^{\mathrm{nr}} \approx 20$\,pH, we see direct fan-in with this circuit design is limited to about 20. This limit can be straightforwardly overcome with an intermediate transformer collection coil, as shown in Fig.\,\ref{fig:fan-in__circuits}(b).

\begin{equation}
\label{eq:fan-in__transformer_collection}
\frac{\Phi_{\mathrm{a}}^{\mathrm{nr}}}{I_0^\mathrm{si}} \approx \frac{ M^{\mathrm{si|nt}} \, M^{\mathrm{nt|nr}} }{L^{\mathrm{nt1}}} \left( 1-\epsilon \right) + \mathcal{O}(\epsilon^2)
\end{equation}
$M^{\mathrm{si|nt}} = k\sqrt{L^{\mathrm{si2}} \, L^{\mathrm{nt1}}}$ and $M^{\mathrm{nt|nr}} = k\sqrt{L^{\mathrm{nt3}} \, L^{\mathrm{nr1}}}$

\begin{equation}
\label{eq:fan-in__transformer_collection__epsilon}
\epsilon \equiv \frac{ L^{\mathrm{nt2}}+L^{\mathrm{nt3}} }{ N L^{\mathrm{nt1}} }
\end{equation}


\bibliographystyle{unsrt}
\bibliography{phenomenological_modeling}

\end{document}